{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"bab36816-bb87-416a-8b14-ef8d33a81fd5","_cell_guid":"7cd6ad0b-582c-43e1-8e53-dc5a3f55a7f4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-03T04:12:11.191393Z","iopub.execute_input":"2021-11-03T04:12:11.191742Z","iopub.status.idle":"2021-11-03T04:12:11.213751Z","shell.execute_reply.started":"2021-11-03T04:12:11.191645Z","shell.execute_reply":"2021-11-03T04:12:11.213100Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#-- required imports\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers as rg\nimport librosa\nfrom librosa import display\nfrom scipy.io import wavfile\nimport gc\nimport pickle as pkl\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\n\npath = '/kaggle/input/darpa-timit-acousticphonetic-continuous-speech'\ndata_path = path+\"/data\"","metadata":{"_uuid":"58c27144-cc7e-49e3-af1e-06340b6fb8a3","_cell_guid":"088cfc22-c1a7-41a7-a758-489471172eaf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-03T04:12:11.215701Z","iopub.execute_input":"2021-11-03T04:12:11.216241Z","iopub.status.idle":"2021-11-03T04:12:19.084557Z","shell.execute_reply.started":"2021-11-03T04:12:11.216190Z","shell.execute_reply":"2021-11-03T04:12:19.083533Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Callback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs={}):\n        print(\"Epoch \",epoch)\n\n    def on_epoch_end(self,epoch,logs={}):\n        print('loss: {:.2f}, accuracy:{:.2f}'.format(\n                logs[\"loss\"],logs[\"accuracy\"]*100))\n        print(logs)\n        gc.collect()\n\n    def on_batch_end(self,batch,logs={}):\n        if(batch%100 == 0):\n            print(batch,'loss: {:.2f}, accuracy:{:.2f}'.format(\n                logs[\"loss\"],logs[\"accuracy\"]*100))\n\n    def on_test_batch_end(self, batch, logs=None):        \n        if(batch%100 == 0):\n            pass\n            return\n            #print('Test Batch',batch,logs)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T04:12:19.086224Z","iopub.execute_input":"2021-11-03T04:12:19.086575Z","iopub.status.idle":"2021-11-03T04:12:19.096496Z","shell.execute_reply.started":"2021-11-03T04:12:19.086532Z","shell.execute_reply":"2021-11-03T04:12:19.094195Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import math\n\nclass CNN_ASR_MODULE_BUILDER():\n    __train_desc = 'train_data.csv'\n    __test_desc = 'test_data.csv'\n    __data_directory = './data'\n    __main_directory = './'\n    f_Path = 'path_from_data_dir' #field that contains file path in train_data.csv\n    f_IsAudio = 'is_converted_audio' #boolean field that tells that the record in train_data.csv contains the description of audio file we are interested in\n    f_IsWord = 'is_word_file'\n    f_IsPhon = 'is_phonetic_file'\n    f_IsSent = 'is_sentence_file'\n    # f_filename = 'filename' #field that contains filename\n    f_dr = 'dialect_region' #field that contains dialect_region information\n    _winlen = 0.025\n    _winstep = 0.01\n    \n    def __init__(self,path=None):\n        if path == None:\n            raise Exception(\"Directory path to the TIMIT Data set must be provided\")\n        if not os.path.isdir(path):\n            raise Exception(\"Directory doesn't exist\")\n        self.__main_directory = path\n        if path[len(path)-1] == '/':\n            self.__data_directory = path+\"data/\"\n        else:\n            self.__main_directory += \"/\"\n            self.__data_directory = self.__main_directory+\"data/\"\n      \n        # TimitBet 61 phoneme mapping to 39 phonemes\n        # by Lee, K.-F., & Hon, H.-W. (1989). Speaker-independent phone recognition using hidden Markov models. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(11), 1641â€“1648. doi:10.1109/29.46546 \n        self.phon61_map39 = {\n            'iy':'iy',  'ih':'ih',   'eh':'eh',  'ae':'ae',    'ix':'ih',  'ax':'ah',   'ah':'ah',  'uw':'uw',\n            'ux':'uw',  'uh':'uh',   'ao':'aa',  'aa':'aa',    'ey':'ey',  'ay':'ay',   'oy':'oy',  'aw':'aw',\n            'ow':'ow',  'l':'l',     'el':'l',  'r':'r',      'y':'y',    'w':'w',     'er':'er',  'axr':'er',\n            'm':'m',    'em':'m',     'n':'n',    'nx':'n',     'en':'n',  'ng':'ng',   'eng':'ng', 'ch':'ch',\n            'jh':'jh',  'dh':'dh',   'b':'b',    'd':'d',      'dx':'dx',  'g':'g',     'p':'p',    't':'t',\n            'k':'k',    'z':'z',     'zh':'sh',  'v':'v',      'f':'f',    'th':'th',   's':'s',    'sh':'sh',\n            'hh':'hh',  'hv':'hh',   'pcl':'h#', 'tcl':'h#', 'kcl':'h#', 'qcl':'h#','bcl':'h#','dcl':'h#',\n            'gcl':'h#','h#':'h#',  '#h':'h#',  'pau':'h#', 'epi': 'h#','nx':'n',   'ax-h':'ah','q':'h#' \n        }\n        \n        self.phon61 = list(self.phon61_map39.keys())\n        self.phon39 = list(set(self.phon61_map39.values()))\n\n        self.label_p39 = {}\n        self.p39_label = {}\n        for i,p in enumerate(self.phon39):\n            self.label_p39[p] = i+1\n            self.p39_label[i+1] = p\n\n        self.phon39_map61 = {}\n        for p61,p39 in self.phon61_map39.items():\n            if not p39 in self.phon39_map61:\n                self.phon39_map61[p39] = []\n            self.phon39_map61[p39].append(p61)\n        #-------------------------------------------------\n        #end __init__\n    \n    #------------------------------------------------------------------------\n    def get39EquiOf61(self,p):\n        return self.phon61_map39[self.removePhonStressMarker(p)]\n\n    def removePhonStressMarker(self,phon):\n        phon = phon.replace('1','')\n        phon = phon.replace('2','')\n        return phon\n    \n    def getWindow(self,sr):\n        nfft = 512\n        winlen = self._winlen * sr\n        winstep = self._winstep * sr\n        return nfft,int(winlen),int(winstep)\n\n    def singleTrainingFrameSize(self,sr):\n        return math.floor(sr/4)\n        \n    def readTrainingDataDescriptionCSV(self):\n        file_path = self.__main_directory + 'train_data.csv' #check if train_data.csv is in correct path\n        self._Tdd = pd.read_csv(file_path)\n        # removing NaN entries in the train_data.csv file\n        dr = ['DR1','DR2','DR3','DR4','DR5','DR6','DR7','DR8']\n        self._Tdd = self._Tdd[self._Tdd['dialect_region'].isin(dr)]\n        return self._Tdd\n\n    def readTestingDataDescriptionCSV(self):\n        file_path = self.__main_directory + 'test_data.csv' #check if train_data.csv is in correct path\n        self._tdd = pd.read_csv(file_path)\n        # removing NaN entries in the train_data.csv file\n        dr = ['DR1','DR2','DR3','DR4','DR5','DR6','DR7','DR8']\n        self._tdd = self._tdd[self._tdd['dialect_region'].isin(dr)]\n        return self._tdd\n    \n    def getListAudioFiles(self,of='Train'):\n        if of == 'Train':\n            self.readTrainingDataDescriptionCSV()\n            return self._Tdd[self._Tdd[self.f_IsAudio] == True]\n        if of == 'Test':\n            self.readTestingDataDescriptionCSV()\n            return self._tdd[self._tdd[self.f_IsAudio] == True]\n        \n    def getListPhonemeFiles(self,of='Train'):\n        if of == 'Train':\n            self.readTrainingDataDescriptionCSV()\n            return self._Tdd[self._Tdd[self.f_IsPhon] == True]\n        if of == 'Test':\n            self.readTestingDataDescriptionCSV()\n            return self._tdd[self._tdd[self.f_IsPhon] == True]\n               \n    def readAudio(self,fpath=None,pre_emp = False):\n        if(fpath == None):\n            return np.zeros(1),0\n        \n        fpath = self.__data_directory+fpath\n        if os.path.exists(fpath):\n            S,sr = librosa.load(fpath,sr=None)\n            if pre_emp:\n                S = librosa.effects.preemphasis(S)\n            return S,sr   \n        else:\n            return np.zeros(1),0\n    #-----------------------end readAudio()\n    \n    def readPhon(self,fpath=None):\n        if(fpath == None):\n            raise Exception('phon file path not provided')\n        \n        fpath = self.__data_directory+fpath\n        ph_ = pd.read_csv(fpath,sep=\" \")#,usecols=['start','end','phoneme'])\n        #ph_.columns = ['start','end','phoneme']\n        return ph_\n            \n        pfn = j['filename'].split('.WAV')[0]+'.PHN'\n        p_bar.set_description(f'Working on {j[\"filename\"]} ,index: {c}  ')\n        try:\n            pfp = file_path+pfd[(pfd['filename']==pfn) & (pfd['speaker_id'] == j['speaker_id'])][f_Path].values[0]\n        except:\n            pfp = afp.replace(j['filename'],pfn)\n            \n        ph_ = pd.read_csv(pfp,sep=\" \")#,usecols=['start','end','phoneme'])\n        #ph_.columns = ['start','end','phoneme']\n    #---------------end readPhon()\n        \n    def getFeatureAndLabel(self,ftype='mfsc',audio_path=None,phon_path=None,n_mels=128,delta=False,delta_delta=False):\n        if audio_path == None:\n            raise Exception(\"Path to audio (Wav) file must be provided\")\n        wav,sr = self.readAudio(fpath=audio_path,pre_emp=True)\n        nfft,winlen,winstep = self.getWindow(sr)\n        if(ftype == 'mfsc'):\n            melspec = librosa.feature.melspectrogram(wav,sr=sr,hop_length=winstep,win_length=winlen,n_fft=nfft,n_mels=n_mels)\n        if(ftype == 'mfcc'):\n            melspec = librosa.feature.mfcc(wav,sr=sr,hop_length=winstep,win_length=winlen,n_fft=nfft,n_mfcc=n_mels)\n            \n        db_melspec = librosa.amplitude_to_db(melspec,ref=np.max)\n        \n        mD = None\n        mDD = None\n        if(delta):\n            mD = librosa.feature.delta(db_melspec)\n            if(delta_delta):\n                mDD = librosa.feature.delta(mD)\n        \n        audio_phon_transcription = None\n        if phon_path == None:\n            tmp = audio_path.split('/')\n            phon_path = \"/\".join(tmp[:(len(tmp)-1)])+\"/\"+ tmp[len(tmp)-1].split('.WAV')[0]+\".PHN\"\n            \n        audio_phon_transcription = self.readPhon(phon_path)            \n        time = db_melspec.shape[1]\n        \n        feature_vectors = []\n        db_melspec = db_melspec.T\n        mD = mD.T\n        mDD = mDD.T\n        \n        prev = None\n        first = audio_phon_transcription.columns\n        audio_phon_transcription.columns = ['start','end','phoneme']\n        labels = []\n        for i in range(time):\n            #---collecting feature---\n            feature = np.zeros(n_mels*3)\n            feature[:n_mels] = db_melspec[i]\n            feature[n_mels:n_mels*2] = mD[i]\n            feature[n_mels*2:n_mels*3] = mDD[i]\n            feature_vectors.append(feature)\n            \n            #---collecting phoneme label ---\n            start = winstep * i\n            end = start+winlen\n            diff = start+400\n            phoneme = list(\n                        audio_phon_transcription[\n                            ((audio_phon_transcription['start']<=start) & \n                            ((audio_phon_transcription['end']-start)>=int(winlen/1.5)))\n                            |\n                            ((audio_phon_transcription['start']<=end) & \n                                (audio_phon_transcription['end']>end))  \n                        ].to_dict()['phoneme'].values()\n            )\n            if len(phoneme) == 0:\n                if int(first[1]) > start:\n                    phoneme = first[2]\n                else:\n                    phoneme = prev\n            else:\n                phoneme = phoneme[0]\n            phoneme = self.get39EquiOf61(phoneme)\n            prev = phoneme\n            labels.append(phoneme)\n             \n        return feature_vectors,labels\n                \n    #--------------------end getMelSpectrogramFeatureAndLabel()\n    def prepareLabelsForTraining(self,labels):\n        print('Preparing Labels')\n        label_vector = []\n        p_bar = tqdm(range(len(labels)))\n        c = 0\n        for l in labels:\n            label = [0 for i in range(39)]\n            label[self.label_p39[l]-1] = 1\n            label_vector.append(label)\n            c+=1\n            if c == 500:\n                p_bar.set_description(f'Working on phoneme {l}')\n                p_bar.update(c)\n                c = 0\n           \n        p_bar.set_description(f'Working on phoneme {l}')\n        p_bar.update(c) \n        return label_vector\n    \n    def collectFeatures(self,ft='Train',ftype='mfsc',n_mels=128,delta=False,delta_delta=False):\n        tddA = self.getListAudioFiles(ft)\n        tddA.index = range(tddA.shape[0])\n        feature_vectors = []\n        labels = []\n        \n        p_bar = tqdm(range(tddA.shape[0]))\n        silent_count = 0\n        for i in range(tddA.shape[0]):\n            fv,lv = self.getFeatureAndLabel(ftype=ftype,audio_path=tddA.loc[i][self.f_Path],n_mels=n_mels,delta=delta,delta_delta=delta_delta)\n            p_bar.set_description(f'Working on {tddA.loc[i][self.f_Path]} ,index: {i}  ')\n            p_bar.update()\n            feature_vectors += fv\n            labels += lv\n                   \n        print(f\"length of feature_vectors is {len(feature_vectors)} and length of labels is {len(labels)}\")\n        labels = np.asarray(np.array(self.prepareLabelsForTraining(labels),dtype=object)).astype(np.int16)\n        feature_vectors = np.asarray(np.array(feature_vectors,dtype=object)).astype(np.float32)\n        return feature_vectors,labels\n    #--------------------end collectFeatures   \n        \n    def classTestA(self):\n        gc.collect()\n        tddA = self.getListAudioFiles()\n        feature_vectors, labels = self.getMelSpectrogramFeatureAndLabel(tddA[self.f_Path][0],n_mels=20,delta=True,delta_delta=True)\n        #------------------------------------------\n        wav,sr = self.readAudio(tddA[self.f_Path][0])\n        librosa.display.waveshow(wav,sr=sr)\n        nfft,winlen,winstep = self.getWindow(sr)\n        print(nfft,winlen,winstep)\n        melspec = librosa.feature.melspectrogram(wav,sr=sr,hop_length=winstep,win_length=winlen,n_fft=nfft)\n        db_melspec = librosa.amplitude_to_db(\n            melspec,\n            ref=np.max)\n        msd = librosa.feature.delta(db_melspec)\n        msdd = librosa.feature.delta(msd)\n        \n        fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n        img = display.specshow(db_melspec,y_axis='linear', x_axis='time',\n                               sr=sr, ax=ax)\n        ax.set(title='Linear-frequency power spectrogram')\n        ax.label_outer()\n        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n        \n        fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n        librosa.display.specshow(db_melspec, y_axis='log', sr=sr,\n                         x_axis='time', ax=ax)\n        ax.set(title='Log-frequency power spectrogram')\n        ax.label_outer()\n        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n        \n        fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n        librosa.display.specshow(msd ,\n                                 y_axis='linear', sr=sr,\n                                 x_axis='time', ax=ax)  \n        ax.set(title='Mel Spectrogram Delta')\n        ax.label_outer()\n        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n        \n        fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n        librosa.display.specshow(msdd ,\n                                 y_axis='linear', sr=sr,\n                                 x_axis='time', ax=ax)  \n        ax.set(title='Mel Spectrogram Delta Delta')\n        ax.label_outer()\n        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n        \n        fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n        librosa.display.specshow(librosa.amplitude_to_db(librosa.feature.mfcc(S=melspec,sr=sr),ref=np.max) ,\n                                 y_axis='linear', sr=sr,\n                                 x_axis='time', ax=ax)  \n        ax.set(title='MFCC')\n        ax.label_outer()\n        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n        \n         ","metadata":{"execution":{"iopub.status.busy":"2021-11-03T04:12:19.099483Z","iopub.execute_input":"2021-11-03T04:12:19.100379Z","iopub.status.idle":"2021-11-03T04:12:19.156806Z","shell.execute_reply.started":"2021-11-03T04:12:19.100329Z","shell.execute_reply":"2021-11-03T04:12:19.155889Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"####--------------Collecting Training Features----------------------###   \ngc.collect()\ncm = CNN_ASR_MODULE_BUILDER(path)\nn_mels = 64\ndelta = True\ndelta_delta=True\nftype = 'mfsc'\nfeature_path = '/kaggle/input/timit-{}mel-spectrogramdelta-features/'.format(n_mels)#'/kaggle/input/timit-{}mfcc-and-delta-feature/'.format(n_mels)#\n\nprint('Attempting to read features file',feature_path)\nif os.path.exists(feature_path+'features.pkl') or os.path.exists('/kaggle/working/features.pkl'):\n    if os.path.exists(feature_path+'features.pkl'):\n        print(\"-from input\")\n        ffp = open(feature_path+'features.pkl','rb')\n        flp = open(feature_path+'labels.pkl','rb')   \n    elif os.path.exists('/kaggle/working/features.pkl'):\n        print(\"-from output\")\n        ffp = open('/kaggle/working/features.pkl','rb')\n        flp = open('/kaggle/working/labels.pkl','rb')\n    features = pkl.load(ffp)\n    labels = pkl.load(flp)\n    ffp.close()\n    flp.close()\n    features = np.asarray(features).astype(np.float32)\n    labels = np.asarray(labels).astype(np.int16)\n    print(features.shape,labels.shape)\n    print('---- success')\n    #-------\nelse:            \n    print('--- Failed')\n    print('Collecting Features from Audio Files')\n    features,labels = cm.collectFeatures(ftype=ftype,n_mels=n_mels,delta=delta,delta_delta=delta_delta)\n    # -------------\n    ffp = open(\"/kaggle/working/features.pkl\",'wb')\n    pkl.dump(features,ffp)\n    flp = open(\"/kaggle/working/labels.pkl\",'wb')\n    pkl.dump(labels,flp)            \n    ffp.close()\n    flp.close()\n    print('--- Completed')\n    #-------","metadata":{"execution":{"iopub.status.busy":"2021-11-03T04:12:19.158317Z","iopub.execute_input":"2021-11-03T04:12:19.158963Z","iopub.status.idle":"2021-11-03T04:12:28.964774Z","shell.execute_reply.started":"2021-11-03T04:12:19.158920Z","shell.execute_reply":"2021-11-03T04:12:28.963918Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Attempting to read features file /kaggle/input/timit-64mel-spectrogramdelta-features/\n-from input\n(1421707, 192) (1421707, 39)\n---- success\n","output_type":"stream"}]},{"cell_type":"code","source":"####--------------Model Training----------------------###   \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(units=1024, input_shape=[n_mels*3],activation=tf.nn.relu),\n    tf.keras.layers.Dense(units=1024,activation=tf.nn.relu),\n    tf.keras.layers.Dense(units=1024,activation=tf.nn.relu),\n    tf.keras.layers.Dense(units=39,activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\ngc.collect()  \nhistory = model.fit(\n    features[:1137000],labels[:1137000],epochs=25,\n     batch_size=512, verbose=1,\n    validation_data=(features[1137000:],labels[1137000:]),\n    validation_batch_size=128\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T04:23:49.573473Z","iopub.execute_input":"2021-11-03T04:23:49.573812Z","iopub.status.idle":"2021-11-03T05:29:13.635253Z","shell.execute_reply.started":"2021-11-03T04:23:49.573766Z","shell.execute_reply":"2021-11-03T05:29:13.634364Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_9 (Dense)              (None, 1024)              197632    \n_________________________________________________________________\ndense_10 (Dense)             (None, 1024)              1049600   \n_________________________________________________________________\ndense_11 (Dense)             (None, 1024)              1049600   \n_________________________________________________________________\ndense_12 (Dense)             (None, 39)                39975     \n=================================================================\nTotal params: 2,336,807\nTrainable params: 2,336,807\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/25\n2221/2221 [==============================] - 160s 72ms/step - loss: 1.7311 - accuracy: 0.5537 - val_loss: 1.4050 - val_accuracy: 0.5798\nEpoch 2/25\n2221/2221 [==============================] - 156s 70ms/step - loss: 1.3067 - accuracy: 0.6039 - val_loss: 1.3008 - val_accuracy: 0.6069\nEpoch 3/25\n2221/2221 [==============================] - 156s 70ms/step - loss: 1.2369 - accuracy: 0.6213 - val_loss: 1.2756 - val_accuracy: 0.6142\nEpoch 4/25\n2221/2221 [==============================] - 155s 70ms/step - loss: 1.1924 - accuracy: 0.6321 - val_loss: 1.2428 - val_accuracy: 0.6238\nEpoch 5/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 1.1568 - accuracy: 0.6419 - val_loss: 1.2352 - val_accuracy: 0.6268\nEpoch 6/25\n2221/2221 [==============================] - 155s 70ms/step - loss: 1.1265 - accuracy: 0.6496 - val_loss: 1.2512 - val_accuracy: 0.6238\nEpoch 7/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 1.1022 - accuracy: 0.6563 - val_loss: 1.2407 - val_accuracy: 0.6293\nEpoch 8/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 1.0776 - accuracy: 0.6628 - val_loss: 1.2617 - val_accuracy: 0.6267\nEpoch 9/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 1.0577 - accuracy: 0.6684 - val_loss: 1.2446 - val_accuracy: 0.6297\nEpoch 10/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 1.0396 - accuracy: 0.6736 - val_loss: 1.2399 - val_accuracy: 0.6339\nEpoch 11/25\n2221/2221 [==============================] - 157s 70ms/step - loss: 1.0268 - accuracy: 0.6772 - val_loss: 1.2956 - val_accuracy: 0.6234\nEpoch 12/25\n2221/2221 [==============================] - 156s 70ms/step - loss: 1.0135 - accuracy: 0.6811 - val_loss: 1.2759 - val_accuracy: 0.6277\nEpoch 13/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 1.0005 - accuracy: 0.6846 - val_loss: 1.2914 - val_accuracy: 0.6286\nEpoch 14/25\n2221/2221 [==============================] - 156s 70ms/step - loss: 0.9909 - accuracy: 0.6878 - val_loss: 1.2969 - val_accuracy: 0.6254\nEpoch 15/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 0.9808 - accuracy: 0.6904 - val_loss: 1.3093 - val_accuracy: 0.6269\nEpoch 16/25\n2221/2221 [==============================] - 156s 70ms/step - loss: 0.9724 - accuracy: 0.6929 - val_loss: 1.3287 - val_accuracy: 0.6271\nEpoch 17/25\n2221/2221 [==============================] - 155s 70ms/step - loss: 0.9649 - accuracy: 0.6953 - val_loss: 1.3365 - val_accuracy: 0.6271\nEpoch 18/25\n2221/2221 [==============================] - 159s 72ms/step - loss: 0.9583 - accuracy: 0.6973 - val_loss: 1.3509 - val_accuracy: 0.6251\nEpoch 19/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 0.9491 - accuracy: 0.6999 - val_loss: 1.3528 - val_accuracy: 0.6280\nEpoch 20/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 0.9449 - accuracy: 0.7009 - val_loss: 1.3542 - val_accuracy: 0.6265\nEpoch 21/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 0.9393 - accuracy: 0.7029 - val_loss: 1.3798 - val_accuracy: 0.6231\nEpoch 22/25\n2221/2221 [==============================] - 160s 72ms/step - loss: 0.9341 - accuracy: 0.7047 - val_loss: 1.3713 - val_accuracy: 0.6270\nEpoch 23/25\n2221/2221 [==============================] - 158s 71ms/step - loss: 0.9294 - accuracy: 0.7062 - val_loss: 1.3983 - val_accuracy: 0.6259\nEpoch 24/25\n2221/2221 [==============================] - 159s 71ms/step - loss: 0.9230 - accuracy: 0.7083 - val_loss: 1.4175 - val_accuracy: 0.6227\nEpoch 25/25\n2221/2221 [==============================] - 157s 71ms/step - loss: 0.9191 - accuracy: 0.7091 - val_loss: 1.4228 - val_accuracy: 0.6207\n","output_type":"stream"}]},{"cell_type":"code","source":"###------------collecting test features -------------------\ngc.collect()\nif os.path.exists(feature_path+'test_features.pkl'):\n    test_features = pkl.load(open(feature_path+'test_features.pkl','rb'))\n    test_labels = pkl.load(open(feature_path+'test_labels.pkl','rb'))\nelif os.path.exists(\"/kaggle/working/test_features.pkl\"):\n    test_features = pkl.load(open(\"/kaggle/working/test_features.pkl\",'rb'))\n    test_labels = pkl.load(open(\"/kaggle/working/test_labels.pkl\",'rb'))\nelse:\n    test_features,test_labels = cm.collectFeatures(ft='Test',ftype=ftype,n_mels=n_mels,delta=delta,delta_delta=delta_delta)\n    pkl.dump(test_features,open('/kaggle/working/test_features.pkl','wb'))\n    pkl.dump(test_labels,open('/kaggle/working/test_labels.pkl','wb'))\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T05:29:13.636990Z","iopub.execute_input":"2021-11-03T05:29:13.637695Z","iopub.status.idle":"2021-11-03T05:29:19.826099Z","shell.execute_reply.started":"2021-11-03T05:29:13.637655Z","shell.execute_reply":"2021-11-03T05:29:19.824278Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"####--------------Model Evaluating----------------------###   \nevaluation = model.evaluate(test_features,test_labels,batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T05:29:19.827308Z","iopub.execute_input":"2021-11-03T05:29:19.827547Z","iopub.status.idle":"2021-11-03T05:29:59.023813Z","shell.execute_reply.started":"2021-11-03T05:29:19.827518Z","shell.execute_reply":"2021-11-03T05:29:59.022956Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"4059/4059 [==============================] - 39s 10ms/step - loss: 1.4273 - accuracy: 0.6135\n","output_type":"stream"}]}]}